{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import flexflow.serve as ff\n",
    "import json, os\n",
    "from types import SimpleNamespace\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data comes from https://huggingface.co/datasets/databricks/databricks-dolly-15k\n",
    "def import_dataset(dataset_size=10, inference_percentage=0.6):\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    data = []\n",
    "    for i,row in enumerate(dataset):\n",
    "        if i == dataset_size:\n",
    "            break\n",
    "        if len(row['context']) == 0:\n",
    "            data.append((row['instruction'],row['response']))\n",
    "    inference_prompts = []\n",
    "    finetuning_prompts = []\n",
    "    for d in data:\n",
    "        if random.random() <= inference_percentage:\n",
    "            inference_prompts.append(d[0])\n",
    "        else:\n",
    "            finetuning_prompts.append(d)\n",
    "    return inference_prompts, finetuning_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    # required parameters\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 8192,\n",
    "    \"zero_copy_memory_per_node\": 12000,\n",
    "    # optional parameters\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "}\n",
    "model_configs = {\n",
    "    # required parameters\n",
    "    \"base_model\": \"JackFram/llama-160m\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    # optional parameters\n",
    "    \"cache_path\": \"\",\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"prompt\": \"\",\n",
    "    \"finetuning_dataset\": \"../../rdelacou/peft_dataset.json\",\n",
    "    \"output_file\": \"../output/ff_peft.txt\",\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs = SimpleNamespace(**configs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FlexFlow runtime. ff.init() takes a dictionary or the path to a JSON file with the configs\n",
    "def init_llm(configs_dict, configs):\n",
    "    ff.init(configs_dict)\n",
    "\n",
    "    # Create the FlexFlow LLM\n",
    "    ff_data_type = (\n",
    "        ff.DataType.DT_FLOAT if configs.full_precision else ff.DataType.DT_HALF\n",
    "    )\n",
    "    llm = ff.LLM(\n",
    "        configs.base_model,\n",
    "        data_type=ff_data_type,\n",
    "        cache_path=configs.cache_path,\n",
    "        refresh_cache=configs.refresh_cache,\n",
    "        output_file=configs.output_file,\n",
    "    )\n",
    "    # Add inference and/or finetuning lora\n",
    "    lora_inference_config = None\n",
    "    lora_finetuning_config = None\n",
    "    if len(configs.prompt) > 0:\n",
    "        lora_inference_config = ff.LoraLinearConfig(\n",
    "            llm.cache_path, configs.inference_peft_model_id\n",
    "        )\n",
    "        llm.add_peft(lora_inference_config)\n",
    "    if len(configs.finetuning_dataset) > 0:\n",
    "        # lora_finetuning_config = ff.LoraLinearConfig(\n",
    "        #     llm.cache_path,\n",
    "        #     configs.finetuning_peft_model_id,\n",
    "        #     target_modules=[\"down_proj\"],\n",
    "        #     rank=16,\n",
    "        #     lora_alpha=16,\n",
    "        #     trainable=True,\n",
    "        #     init_lora_weights=True,\n",
    "        #     optimizer_type=ff.OptimizerType.OPTIMIZER_TYPE_SGD,\n",
    "        # )\n",
    "        lora_finetuning_config = ff.LoraLinearConfig(\n",
    "            llm.cache_path,\n",
    "            configs.inference_peft_model_id,\n",
    "            trainable=True,\n",
    "            optimizer_type=ff.OptimizerType.OPTIMIZER_TYPE_SGD,\n",
    "            optimizer_kwargs={\n",
    "                \"learning_rate\": 1.0,\n",
    "                \"momentum\": 0.0,\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"nesterov\": False,\n",
    "            },\n",
    "        )\n",
    "        llm.add_peft(lora_finetuning_config)\n",
    "\n",
    "    # Compile the LLM for inference and load the weights into memory\n",
    "    generation_config = ff.GenerationConfig(\n",
    "        do_sample=False, temperature=0.9, topp=0.8, topk=1\n",
    "    )\n",
    "    llm.compile(\n",
    "        generation_config,\n",
    "        enable_peft_finetuning=(len(configs.finetuning_dataset) > 0),\n",
    "        max_requests_per_batch=1,\n",
    "        max_seq_length=256,\n",
    "        max_tokens_per_batch=128,\n",
    "    )\n",
    "    return llm, lora_inference_config, lora_finetuning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 7f9b6a648280]    0.256053 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f9b6a648280]    0.256113 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f9b6a648280]    0.256122 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f9b6a648280]    0.256129 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f9b6a648280]    0.256135 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "workSpaceSize (128 MB)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm, lora_inference_config, lora_finetuning_config = init_llm(configs_dict, configs)\n",
    "\n",
    "llm.start_server()\n",
    "\n",
    "requests = []\n",
    "# Serving\n",
    "if len(configs.prompt) > 0:\n",
    "    prompts = [s for s in json.load(open(configs.prompt))]\n",
    "    inference_requests = [\n",
    "        ff.Request(\n",
    "            ff.RequestType.REQ_INFERENCE,\n",
    "            prompt=prompt,\n",
    "            max_sequence_length=128,\n",
    "            peft_model_id=llm.get_ff_peft_id(lora_inference_config),\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    requests += inference_requests\n",
    "# Finetuning\n",
    "if len(configs.finetuning_dataset) > 0:\n",
    "    finetuning_request = ff.Request(\n",
    "        ff.RequestType.REQ_FINETUNING,\n",
    "        max_sequence_length=128,\n",
    "        peft_model_id=llm.get_ff_peft_id(lora_finetuning_config),\n",
    "        dataset_filepath=os.path.join(os.getcwd(), configs.finetuning_dataset),\n",
    "        max_training_steps=2,\n",
    "    )\n",
    "    requests.append(finetuning_request)\n",
    "\n",
    "llm.generate(requests)\n",
    "\n",
    "llm.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
