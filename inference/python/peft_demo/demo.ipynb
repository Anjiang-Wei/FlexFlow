{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from datasets import load_dataset\n",
    "from demo_class import FlexFlowDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_size=10, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path-- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if len(data) == dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 8192,\n",
    "    \"zero_copy_memory_per_node\": 12000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"JackFram/llama-160m\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"cache_path\": \"\",\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"prompt\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"finetuning_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 10\n",
      "[0 - 7fe968e43280]    0.401319 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fe968e43280]    0.401630 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fe968e43280]    0.401717 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fe968e43280]    0.401789 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fe968e43280]    0.401857 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[0 - 7fe968e43280]    0.569370 {3}{flexflow_c}: [FFConfig] new 0x8381f90\n",
      "[0 - 7fe968e43280]    0.569650 {3}{flexflow_c}: [RequestManager] get 0xaaa00a0\n",
      "[0 - 7fe968e43280]    0.569733 {3}{flexflow_c}: [RequestManager] set max_requests_per_batch 1\n",
      "[0 - 7fe968e43280]    0.569759 {3}{flexflow_c}: [RequestManager] set max_tokens_per_batch 128\n",
      "[0 - 7fe968e43280]    0.569822 {3}{flexflow_c}: [RequestManager] set max_sequence_length 256\n",
      "[0 - 7fe968e43280]    0.569904 {3}{flexflow_c}: [RequestManager] set_enable_peft_finetuning 1\n",
      "workSpaceSize (128 MB)\n",
      "[0 - 7fe968e43280]    0.769175 {3}{flexflow_c}: [FFModel] new 0xae74380\n",
      "[0 - 7fe968e43280]    0.800721 {3}{flexflow_c}: [Tensor] new 2D 0xa8d6fb0 (1, 128, 0, 0)\n",
      "[0 - 7fe968e43280]    0.827010 {3}{flexflow_c}: [Tensor] get dims [0, 0, 128, 1]\n",
      "[0 - 7fe968e43280]    0.827180 {3}{flexflow_c}: [UniformInitializer] new 0xab89e20\n",
      "[0 - 7fe968e43280]    0.935048 {3}{flexflow_c}: [Embedding] new Tensor 0xaaadc90, input 0xa8d6fb0, num_entries 32000, out_dim 768, aggr 20, dtype 44, shared_op (nil), kernel_init 0xab89e20, name embed_tokens\n",
      "[0 - 7fe968e43280]    0.935209 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.935759 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.936083 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.936261 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.936291 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.936521 {3}{flexflow_c}: [Dense] new Tensor 2D 0xae68d80 (3072, 1, 128, 0), input 0xae689d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.936554 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.936637 {3}{flexflow_c}: [Dense] new Tensor 2D 0xae68c70 (3072, 1, 128, 0), input 0xae689d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.936661 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.936769 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xaaa9530, input1 0xae68d80, input2 0xae68c70, name (null)\n",
      "[0 - 7fe968e43280]    0.936810 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.936927 {3}{flexflow_c}: [Dense] new Tensor 2D 0xae6f9d0 (768, 1, 128, 0), input 0xaaa9530, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.936952 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937048 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937082 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937176 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937259 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937292 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937379 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaabac60 (3072, 1, 128, 0), input 0xaaba980, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.937399 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.937485 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaabab50 (3072, 1, 128, 0), input 0xaaba980, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.937503 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.937554 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xaabafe0, input1 0xaabac60, input2 0xaabab50, name (null)\n",
      "[0 - 7fe968e43280]    0.937575 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.937658 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaabb470 (768, 1, 128, 0), input 0xaabafe0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.937677 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937753 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937786 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937902 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.937979 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938000 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938089 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaaaea0 (3072, 1, 128, 0), input 0xaabbf20, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.938116 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.938196 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaaad90 (3072, 1, 128, 0), input 0xaabbf20, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.938214 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.938272 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xaaab220, input1 0xaaaaea0, input2 0xaaaad90, name (null)\n",
      "[0 - 7fe968e43280]    0.938292 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.938379 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaab6b0 (768, 1, 128, 0), input 0xaaab220, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.938398 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938469 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938491 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938594 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938676 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938707 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.938785 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaac760 (3072, 1, 128, 0), input 0xaaac7c0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.938803 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.938895 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaac990 (3072, 1, 128, 0), input 0xaaac7c0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.938923 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.938962 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xaaacdd0, input1 0xaaac760, input2 0xaaac990, name (null)\n",
      "[0 - 7fe968e43280]    0.938982 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.939065 {3}{flexflow_c}: [Dense] new Tensor 2D 0xaaad260 (768, 1, 128, 0), input 0xaaacdd0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.939090 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939170 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939193 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939284 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939363 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939394 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939477 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb0279c0 (3072, 1, 128, 0), input 0xb033bf0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.939495 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.939573 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb033dc0 (3072, 1, 128, 0), input 0xb033bf0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.939592 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.939635 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb0403a0, input1 0xb0279c0, input2 0xb033dc0, name (null)\n",
      "[0 - 7fe968e43280]    0.939655 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.939741 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb064d10 (768, 1, 128, 0), input 0xb0403a0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.939759 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939834 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939869 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.939963 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940048 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940084 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940164 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb07dca0 (3072, 1, 128, 0), input 0xb089ed0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.940182 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.940255 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb08a0a0 (3072, 1, 128, 0), input 0xb089ed0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.940273 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.940318 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb096680, input1 0xb07dca0, input2 0xb08a0a0, name (null)\n",
      "[0 - 7fe968e43280]    0.940338 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.940422 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb0baff0 (768, 1, 128, 0), input 0xb096680, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.940442 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940527 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940549 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940653 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940723 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940745 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.940824 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb0d43f0 (3072, 1, 128, 0), input 0xb0e05f0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.940864 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.940951 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb0e07c0 (3072, 1, 128, 0), input 0xb0e05f0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.940972 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.941026 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb0ecda0, input1 0xb0d43f0, input2 0xb0e07c0, name (null)\n",
      "[0 - 7fe968e43280]    0.941057 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.941127 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb111710 (768, 1, 128, 0), input 0xb0ecda0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.941145 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941228 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941260 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941355 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941423 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941455 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941530 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb12a6a0 (3072, 1, 128, 0), input 0xb1368d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.941549 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.941624 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb136aa0 (3072, 1, 128, 0), input 0xb1368d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.941642 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.941695 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb143080, input1 0xb12a6a0, input2 0xb136aa0, name (null)\n",
      "[0 - 7fe968e43280]    0.941715 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.941790 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb1679f0 (768, 1, 128, 0), input 0xb143080, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.941807 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941903 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.941937 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942030 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942100 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942130 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942209 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb180980 (3072, 1, 128, 0), input 0xb18cbb0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.942225 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.942296 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb18cd80 (3072, 1, 128, 0), input 0xb18cbb0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.942313 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.942356 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb199360, input1 0xb180980, input2 0xb18cd80, name (null)\n",
      "[0 - 7fe968e43280]    0.942376 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.942457 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb1bdcd0 (768, 1, 128, 0), input 0xb199360, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.942474 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942553 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942575 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942673 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942750 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942782 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.942871 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb1d6c60 (3072, 1, 128, 0), input 0xb1e2e90, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.942889 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.942969 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb1e3060 (3072, 1, 128, 0), input 0xb1e2e90, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.942987 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.943039 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb1ef640, input1 0xb1d6c60, input2 0xb1e3060, name (null)\n",
      "[0 - 7fe968e43280]    0.943059 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.943140 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb213fb0 (768, 1, 128, 0), input 0xb1ef640, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.943160 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943243 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943276 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943367 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943448 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943482 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943560 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb22cf40 (3072, 1, 128, 0), input 0xb239170, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.943579 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.943653 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb239340 (3072, 1, 128, 0), input 0xb239170, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.943670 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.943721 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb245920, input1 0xb22cf40, input2 0xb239340, name (null)\n",
      "[0 - 7fe968e43280]    0.943740 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.943827 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb26a290 (768, 1, 128, 0), input 0xb245920, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.943853 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943923 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.943953 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944036 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944116 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944147 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944225 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb283220 (3072, 1, 128, 0), input 0xb28f450, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.gate_proj\n",
      "[0 - 7fe968e43280]    0.944241 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.944319 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb28f620 (3072, 1, 128, 0), input 0xb28f450, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.up_proj\n",
      "[0 - 7fe968e43280]    0.944335 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.944377 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xb29bc00, input1 0xb283220, input2 0xb28f620, name (null)\n",
      "[0 - 7fe968e43280]    0.944397 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7fe968e43280]    0.944469 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb2c0570 (768, 1, 128, 0), input 0xb29bc00, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.down_proj\n",
      "[0 - 7fe968e43280]    0.944485 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944565 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944595 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7fe968e43280]    0.944674 {3}{flexflow_c}: [Dense] new Tensor 2D 0xb0bb3f0 (32000, 1, 128, 0), input 0xb2cca50, out_dim 32000, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name lm_head\n",
      "[0 - 7fe968e43280]    0.944691 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 32000]\n",
      "[0 - 7fe968e43280]    0.944808 {3}{flexflow_c}: [Softmax] new Tensor 0xb0bb770, input 0xb0bb3f0, name (null)\n",
      "[0 - 7fe968e43280]    0.944845 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 32000]\n",
      "[0 - 7fe968e43280]    0.944992 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 1]\n",
      "[0 - 7fe968e43280]    0.948211 {3}{flexflow_c}: [LoraLinearConfig] new 0xadfd510\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n",
      "[0 - 7fe968e43280]    0.948710 {3}{flexflow_c}: [Add Lora Layer] model handle: 0xae74380, peft_config handle 0xadfd510, peft_model_id: 0xab7cd90\n",
      "[0 - 7fe968e43280]    0.948939 {3}{flexflow_c}: [LoraLinearConfig] new 0xad79250\n",
      "[0 - 7fe968e43280]    0.950937 {3}{flexflow_c}: [Add Lora Layer] model handle: 0xae74380, peft_config handle 0xad79250, peft_model_id: 0xaccf7b0\n",
      "[0 - 7fe968e43280]    1.074273 {3}{flexflow_c}: [RequestManager] set max_spec_tree_token_num 20\n",
      "[0 - 7fe968e43280]    1.074487 {3}{flexflow_c}: [FileDataLoader] new 0xb2f3330\n",
      "[0 - 7fe968e43280]    1.074622 {3}{flexflow_c}: [InferenceManager] get 0xb2f2f10\n",
      "[0 - 7fe968e43280]    1.074698 {3}{flexflow_c}: [InferenceManager] register_model_weights_loader 0xb2f2f10 0xae74380 0xb2f3330\n",
      "Loading tokenizer...\n",
      "[0 - 7fe968e43280]    1.124180 {3}{flexflow_c}: [RequestManager] register tokenizer 0xaaa00a0 /root/.cache/flexflow/tokenizers/jackfram/llama-160m\n",
      "[0 - 7fe968e43280]    1.124309 {3}{flexflow_c}: [RequestManager] register output filepath 0xaaa00a0 peft_demo.txt\n",
      "[0 - 7fe968e43280]    1.124633 {3}{flexflow_c}: [RequestManager] start background server 0xaaa00a0 0xae74380\n",
      "Background server started.\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7fe904bfa990>]\n",
      "[0 - 7fe968e43280]    1.126110 {3}{flexflow_c}: [Model] generate[0] 0xae74380 /usr/FlexFlow/inference/python/peft_demo/finetuning_dataset.json 128 100\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7fe968e43280]    1.127612 {3}{RequestManager}: [0] input: 1 3750 508 3949 1379 10503 573 363 1472 1728 4094 29973 5500 1379 671 278 9950 297 1009 3165 567 304 3013 963 10423 411 5864 322 27246 29878 362 363 1472 23704 310 931 29889\n",
      "[0 - 7fe968e43280]    1.127620 {3}{RequestManager}: [0] output:\n",
      "[0 - 7fe968e43280]    1.127633 {3}{RequestManager}: [1] input: 1 16308 29915 29879 11825 505 2211 29215 29901 28533 29892 23010 29891 29892 322 825 30010 29879 278 1024 310 278 4654 8750 29973 450 1024 310 278 4654 8750 338 16308\n",
      "[0 - 7fe968e43280]    1.127635 {3}{RequestManager}: [1] output:\n",
      "[0 - 7fe968e43280]    1.127642 {3}{RequestManager}: [2] input: 1 11644 4846 278 8291 278 2982 297 23526 304 2048 1009 379 29984 2259 360 1528 4937 29888 4539\n",
      "[0 - 7fe968e43280]    1.127644 {3}{RequestManager}: [2] output:\n",
      "[0 - 7fe968e43280]    1.127704 {3}{RequestManager}: [3] input: 1 1724 338 263 29807 29973 319 29807 338 263 883 297 1879 7843 29889 29871 739 338 263 2323 22112 10694 1754 310 16791 3454 322 738 1353 310 13791 29889 29871 739 338 263 5764 9704 310 6631 1196 24611 470 12770 29889 29871 450 13791 310 278 29807 526 8429 988 1023 12770 5870 29889 29871 1222 9422 310 1248 4790 787 526 15090 351 787 29892 11137 351 787 29892 322 4725 351 787 29889 29871 3139 10694 393 947 451 1712 12770 470 13791 338 451 263 29807 29889 29871 530 1342 310 263 1661 29899 3733 17125 338 263 8607 29889\n",
      "[0 - 7fe968e43280]    1.127709 {3}{RequestManager}: [3] output:\n",
      "[0 - 7fe968e43280]    1.127724 {3}{RequestManager}: [4] input: 1 8449 23238 310 4259 3023 310 8448 310 498 1617 267 1258 3375 1808 4326 29931 8326 1513 29973 2296 10624 376 29949 493 23935 29908 322 376 6730 310 3600 4408 29908 278 11582 322 18615 23238 310 4259 3023 29892 8307 29889\n",
      "[0 - 7fe968e43280]    1.127726 {3}{RequestManager}: [4] output:\n",
      "[0 - 7fe968e43280]    1.127738 {3}{RequestManager}: [5] input: 1 1724 5375 756 2113 278 1556 19025 7684 1612 1338 297 278 4955 310 278 8090 29973 5765 1963 295 567 756 2113 278 1556 7684 1612 1338 310 599 931 411 29871 29906 29941 330 3361 29889\n",
      "[0 - 7fe968e43280]    1.127741 {3}{RequestManager}: [5] output:\n",
      "[0 - 7fe968e43280]    1.127748 {3}{RequestManager}: [6] input: 1 8449 14872 7664 23139 1346 29954 5168 411 263 21265 29880 382 23693 30024 29973 478 10877 261\n",
      "[0 - 7fe968e43280]    1.127750 {3}{RequestManager}: [6] output:\n",
      "[0 - 7fe968e43280]    1.127757 {3}{RequestManager}: [7] input: 1 1724 5930 746 278 6575 5771 1623 29973 1932 278 6575 6166 29892 278 11005 8665 29889\n",
      "[0 - 7fe968e43280]    1.127759 {3}{RequestManager}: [7] output:\n",
      "[0 - 7fe968e43280]    1.127792 {3}{RequestManager}: [8] input: 1 1724 338 263 9750 29973 319 9750 338 385 3158 1734 393 16612 385 6354 29889 29871 1222 9422 310 1147 5824 526 278 1494 29901 2381 25217 29892 298 638 292 29892 289 638 292 29892 9679 261 2071 1218 29892 470 2071 2941 4357 29889 29871 2178 310 1438 2323 322 10296 1734 6455 526 21351 304 385 6354 393 738 8471 2655 508 437 29889 29871 1152 1342 29892 263 11203 508 4768 446 22203 411 263 5199 746 278 5199 338 8939 12818 278 4768 446 29889 29871 26646 671 338 451 9078 304 25618 470 2305 871 541 16058 304 599 8471 2712 29889\n",
      "[0 - 7fe968e43280]    1.127795 {3}{RequestManager}: [8] output:\n",
      "[0 - 7fe968e43280]    1.127805 {3}{RequestManager}: [9] input: 1 11644 3897 6989 310 20262 297 29871 29896 29947 29900 29953 29973 4667 306 310 278 24553 3897 6989 310 20262 297 29871 29896 29947 29900 29953 29889\n",
      "[0 - 7fe968e43280]    1.127808 {3}{RequestManager}: [9] output:\n",
      "2024-07-17 15:01:55 - ###PEFT DEBUGGING### Starting background serving task.\n",
      "2024-07-17 15:01:55 - ###PEFT DEBUGGING### Updated models' configuration.\n",
      "###PEFT DEBUGGING### LLM Model object exists.\n",
      "###PEFT DEBUGGING### Model object exists.\n",
      "###PEFT DEBUGGING### Model object still exists.\n",
      "###PEFT DEBUGGING### Entering compile_inference.\n",
      "###PEFT DEBUGGING### Configuration check passed: At least four CPU cores per node.\n",
      "###PEFT DEBUGGING### Launching graph optimization task.\n",
      "num_nodes = 1 num_gpus_per_node = 1\n",
      "optimal_views.size = 102\n",
      "views.size() = 102\n",
      "###PEFT DEBUGGING### Operators reconstructed from optimized graph.\n",
      "###PEFT DEBUGGING### Starting inplace optimizations.\n",
      "###PEFT DEBUGGING### Mapping output tensors.\n",
      "ndim(1) dims[1 0 0 0]\n",
      "###PEFT DEBUGGING### Setting up NCCL communications.\n",
      "###PEFT DEBUGGING### compile_inference completed successfully.\n",
      "Loading weight file embed_tokens.weight\n",
      "Loading weight file layers.0.input_layernorm.weight\n",
      "Loading weight file layers.0.self_attn.q_proj.weight\n",
      "Loading weight file layers.0.self_attn.k_proj.weight\n",
      "Loading weight file layers.0.self_attn.v_proj.weight\n",
      "Loading weight file layers.0.self_attn.o_proj.weight\n",
      "Loading weight file layers.0.post_attention_layernorm.weight\n",
      "Loading weight file layers.0.mlp.gate_proj.weight\n",
      "Loading weight file layers.0.mlp.up_proj.weight\n",
      "Loading weight file layers.0.mlp.down_proj.weight\n",
      "Loading weight file layers.1.input_layernorm.weight\n",
      "Loading weight file layers.1.self_attn.q_proj.weight\n",
      "Loading weight file layers.1.self_attn.k_proj.weight\n",
      "Loading weight file layers.1.self_attn.v_proj.weight\n",
      "Loading weight file layers.1.self_attn.o_proj.weight\n",
      "Loading weight file layers.1.post_attention_layernorm.weight\n",
      "Loading weight file layers.1.mlp.gate_proj.weight\n",
      "Loading weight file layers.1.mlp.up_proj.weight\n",
      "Loading weight file layers.1.mlp.down_proj.weight\n",
      "Loading weight file layers.2.input_layernorm.weight\n",
      "Loading weight file layers.2.self_attn.q_proj.weight\n",
      "Loading weight file layers.2.self_attn.k_proj.weight\n",
      "Loading weight file layers.2.self_attn.v_proj.weight\n",
      "Loading weight file layers.2.self_attn.o_proj.weight\n",
      "Loading weight file layers.2.post_attention_layernorm.weight\n",
      "Loading weight file layers.2.mlp.gate_proj.weight\n",
      "Loading weight file layers.2.mlp.up_proj.weight\n",
      "Loading weight file layers.2.mlp.down_proj.weight\n",
      "Loading weight file layers.3.input_layernorm.weight\n",
      "Loading weight file layers.3.self_attn.q_proj.weight\n",
      "Loading weight file layers.3.self_attn.k_proj.weight\n",
      "Loading weight file layers.3.self_attn.v_proj.weight\n",
      "Loading weight file layers.3.self_attn.o_proj.weight\n",
      "Loading weight file layers.3.post_attention_layernorm.weight\n",
      "Loading weight file layers.3.mlp.gate_proj.weight\n",
      "Loading weight file layers.3.mlp.up_proj.weight\n",
      "Loading weight file layers.3.mlp.down_proj.weight\n",
      "Loading weight file layers.4.input_layernorm.weight\n",
      "Loading weight file layers.4.self_attn.q_proj.weight\n",
      "Loading weight file layers.4.self_attn.k_proj.weight\n",
      "Loading weight file layers.4.self_attn.v_proj.weight\n",
      "Loading weight file layers.4.self_attn.o_proj.weight\n",
      "Loading weight file layers.4.post_attention_layernorm.weight\n",
      "Loading weight file layers.4.mlp.gate_proj.weight\n",
      "Loading weight file layers.4.mlp.up_proj.weight\n",
      "Loading weight file layers.4.mlp.down_proj.weight\n",
      "Loading weight file layers.5.input_layernorm.weight\n",
      "Loading weight file layers.5.self_attn.q_proj.weight\n",
      "Loading weight file layers.5.self_attn.k_proj.weight\n",
      "Loading weight file layers.5.self_attn.v_proj.weight\n",
      "Loading weight file layers.5.self_attn.o_proj.weight\n",
      "Loading weight file layers.5.post_attention_layernorm.weight\n",
      "Loading weight file layers.5.mlp.gate_proj.weight\n",
      "Loading weight file layers.5.mlp.up_proj.weight\n",
      "Loading weight file layers.5.mlp.down_proj.weight\n",
      "Loading weight file layers.6.input_layernorm.weight\n",
      "Loading weight file layers.6.self_attn.q_proj.weight\n",
      "Loading weight file layers.6.self_attn.k_proj.weight\n",
      "Loading weight file layers.6.self_attn.v_proj.weight\n",
      "Loading weight file layers.6.self_attn.o_proj.weight\n",
      "Loading weight file layers.6.post_attention_layernorm.weight\n",
      "Loading weight file layers.6.mlp.gate_proj.weight\n",
      "Loading weight file layers.6.mlp.up_proj.weight\n",
      "Loading weight file layers.6.mlp.down_proj.weight\n",
      "Loading weight file layers.7.input_layernorm.weight\n",
      "Loading weight file layers.7.self_attn.q_proj.weight\n",
      "Loading weight file layers.7.self_attn.k_proj.weight\n",
      "Loading weight file layers.7.self_attn.v_proj.weight\n",
      "Loading weight file layers.7.self_attn.o_proj.weight\n",
      "Loading weight file layers.7.post_attention_layernorm.weight\n",
      "Loading weight file layers.7.mlp.gate_proj.weight\n",
      "Loading weight file layers.7.mlp.up_proj.weight\n",
      "Loading weight file layers.7.mlp.down_proj.weight\n",
      "Loading weight file layers.8.input_layernorm.weight\n",
      "Loading weight file layers.8.self_attn.q_proj.weight\n",
      "Loading weight file layers.8.self_attn.k_proj.weight\n",
      "Loading weight file layers.8.self_attn.v_proj.weight\n",
      "Loading weight file layers.8.self_attn.o_proj.weight\n",
      "Loading weight file layers.8.post_attention_layernorm.weight\n",
      "Loading weight file layers.8.mlp.gate_proj.weight\n",
      "Loading weight file layers.8.mlp.up_proj.weight\n",
      "Loading weight file layers.8.mlp.down_proj.weight\n",
      "Loading weight file layers.9.input_layernorm.weight\n",
      "Loading weight file layers.9.self_attn.q_proj.weight\n",
      "Loading weight file layers.9.self_attn.k_proj.weight\n",
      "Loading weight file layers.9.self_attn.v_proj.weight\n",
      "Loading weight file layers.9.self_attn.o_proj.weight\n",
      "Loading weight file layers.9.post_attention_layernorm.weight\n",
      "Loading weight file layers.9.mlp.gate_proj.weight\n",
      "Loading weight file layers.9.mlp.up_proj.weight\n",
      "Loading weight file layers.9.mlp.down_proj.weight\n",
      "Loading weight file layers.10.input_layernorm.weight\n",
      "Loading weight file layers.10.self_attn.q_proj.weight\n",
      "Loading weight file layers.10.self_attn.k_proj.weight\n",
      "Loading weight file layers.10.self_attn.v_proj.weight\n",
      "Loading weight file layers.10.self_attn.o_proj.weight\n",
      "Loading weight file layers.10.post_attention_layernorm.weight\n",
      "Loading weight file layers.10.mlp.gate_proj.weight\n",
      "Loading weight file layers.10.mlp.up_proj.weight\n",
      "Loading weight file layers.10.mlp.down_proj.weight\n",
      "Loading weight file layers.11.input_layernorm.weight\n",
      "Loading weight file layers.11.self_attn.q_proj.weight\n",
      "Loading weight file layers.11.self_attn.k_proj.weight\n",
      "Loading weight file layers.11.self_attn.v_proj.weight\n",
      "Loading weight file layers.11.self_attn.o_proj.weight\n",
      "Loading weight file layers.11.post_attention_layernorm.weight\n",
      "Loading weight file layers.11.mlp.gate_proj.weight\n",
      "Loading weight file layers.11.mlp.up_proj.weight\n",
      "Loading weight file layers.11.mlp.down_proj.weight\n",
      "Loading weight file norm.weight\n",
      "Loading weight file lm_head.weight\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "[0 - 7fe8fc019740]   17.559949 {3}{RequestManager}: [Finetuning] guid(1000000) completed_training_steps(100) processed_finetuning_tokens(4570) latency(17559902.0)\n",
      "[0 - 7fe968e43280]   17.561298 {3}{flexflow_c}: [RequestManager] terminate background server 0xaaa00a0\n",
      "2024-07-17 15:02:12 - ###PEFT DEBUGGING### Background serving task completed.\n",
      "Background server stopped.\n"
     ]
    }
   ],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"prompt\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "demo = FlexFlowDemo(configs_dict)\n",
    "\n",
    "demo.initialize_flexflow()\n",
    "demo.start_server()\n",
    "demo.generate_finetuning()\n",
    "demo.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
