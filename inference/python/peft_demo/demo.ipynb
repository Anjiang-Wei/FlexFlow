{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from datasets import load_dataset\n",
    "from demo_class import FlexFlowDemo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(finetune_dataset_size=4, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path -- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if len(data) == finetune_dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 8192,\n",
    "    \"zero_copy_memory_per_node\": 12000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 4, # This value should be greater or equal to the finetuning dataset size\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"JackFram/llama-160m\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"cache_path\": os.environ.get(\"FF_CACHE_PATH\", \"\"),\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"finetuning_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 7fef4d3aa280]    0.235511 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fef4d3aa280]    0.235578 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fef4d3aa280]    0.235590 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fef4d3aa280]    0.235601 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fef4d3aa280]    0.235609 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "workSpaceSize (128 MB)\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"inference_dataset\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "demo = FlexFlowDemo(configs_dict)\n",
    "\n",
    "demo.initialize_flexflow()\n",
    "demo.start_server()\n",
    "#demo.generate_inference()\n",
    "demo.generate_finetuning()\n",
    "#demo.generate_inference()\n",
    "demo.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
