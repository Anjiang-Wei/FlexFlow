{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random, subprocess\n",
    "from datasets import load_dataset\n",
    "from demo_class import FlexFlowDemo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(finetune_dataset_size=2, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path -- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if len(data) == finetune_dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=2, separators=(',', ': '))\n",
    "\n",
    "def download_models_used(base_model=\"meta-llama/Meta-Llama-3-8B\", peft_model=\"goliaro/llama-3-8b-lora\", refresh_cache=True):\n",
    "    args = [peft_model, '--base_model_name', base_model]\n",
    "    if refresh_cache:\n",
    "        args.append('--refresh-cache')\n",
    "    subprocess.run(['python', '../../utils/download_peft_model.py'] + args, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 4,\n",
    "    \"memory_per_gpu\": 14000,\n",
    "    \"zero_copy_memory_per_node\": 40000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 4,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 4,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"cache_path\": os.environ.get(\"FF_CACHE_PATH\", \"\"),\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"finetuning_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 7fec72884280]    0.300277 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300335 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300351 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300375 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300387 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300400 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300411 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7fec72884280]    0.300421 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "workSpaceSize (128 MB)\n",
      "workSpaceSize (128 MB)\n",
      "workSpaceSize (128 MB)\n",
      "workSpaceSize (128 MB)\n",
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Loading tokenizer...\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n",
      "Adding layer layers.12.mlp.down_proj.lora\n",
      "Adding layer layers.13.mlp.down_proj.lora\n",
      "Adding layer layers.14.mlp.down_proj.lora\n",
      "Adding layer layers.15.mlp.down_proj.lora\n",
      "Adding layer layers.16.mlp.down_proj.lora\n",
      "Adding layer layers.17.mlp.down_proj.lora\n",
      "Adding layer layers.18.mlp.down_proj.lora\n",
      "Adding layer layers.19.mlp.down_proj.lora\n",
      "Adding layer layers.20.mlp.down_proj.lora\n",
      "Adding layer layers.21.mlp.down_proj.lora\n",
      "Adding layer layers.22.mlp.down_proj.lora\n",
      "Adding layer layers.23.mlp.down_proj.lora\n",
      "Adding layer layers.24.mlp.down_proj.lora\n",
      "Adding layer layers.25.mlp.down_proj.lora\n",
      "Adding layer layers.26.mlp.down_proj.lora\n",
      "Adding layer layers.27.mlp.down_proj.lora\n",
      "Adding layer layers.28.mlp.down_proj.lora\n",
      "Adding layer layers.29.mlp.down_proj.lora\n",
      "Adding layer layers.30.mlp.down_proj.lora\n",
      "Adding layer layers.31.mlp.down_proj.lora\n",
      "Background server started.\n",
      "2024-07-21 02:29:25 - ###PEFT DEBUGGING### Starting background serving task.\n",
      "2024-07-21 02:29:25 - ###PEFT DEBUGGING### Updated models' configuration.\n",
      "###PEFT DEBUGGING### LLM Model object exists.\n",
      "###PEFT DEBUGGING### Model object exists.\n",
      "###PEFT DEBUGGING### Model object still exists.\n",
      "###PEFT DEBUGGING### Entering compile_inference.\n",
      "###PEFT DEBUGGING### Configuration check passed: At least four CPU cores per node.\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7fec147f89d0>, <flexflow.core.flexflow_cffi.Request object at 0x7fec13dd4950>]\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7fec72884280]    1.352199 {3}{RequestManager}: [0] input: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30 8215 2053 1005 279 8834 304 872 305 12055 311 2567 1124 10409 449 4907 323 88000 369 1317 18852 315 892 13\n",
      "[0 - 7fec72884280]    1.352211 {3}{RequestManager}: [0] output:\n",
      "[0 - 7fec72884280]    1.352220 {3}{RequestManager}: [1] input: 128000 62786 596 6699 617 2380 30968 25 29793 11 23908 88 11 323 1148 753 279 836 315 279 4948 10003 30 578 836 315 279 4948 10003 374 30505\n",
      "[0 - 7fec72884280]    1.352221 {3}{RequestManager}: [1] output:\n",
      "[0]10445\n",
      "[1]649\n",
      "[2]6730\n",
      "[3]2053\n",
      "[4]18167\n",
      "[5]369\n",
      "[6]1317\n",
      "[7]2085\n",
      "[8]3090\n",
      "[9]30\n",
      "[10]8215\n",
      "[11]2053\n",
      "[12]1005\n",
      "[13]279\n",
      "[14]8834\n",
      "[15]304\n",
      "[16]872\n",
      "[17]305\n",
      "[18]12055\n",
      "[19]311\n",
      "[20]2567\n",
      "[21]1124\n",
      "[22]10409\n",
      "[23]449\n",
      "[24]4907\n",
      "[25]323\n",
      "[26]88000\n",
      "[27]369\n",
      "[28]1317\n",
      "[29]18852\n",
      "[30]315\n",
      "[31]892\n",
      "[32]13\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7fec72884280]    1.352384 {3}{RequestManager}: [1000001]New request tokens: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30 8215 2053 1005 279 8834 304 872 305 12055 311 2567 1124 10409 449 4907 323 88000 369 1317 18852 315 892 13\n",
      "###PEFT DEBUGGING### Launching graph optimization task.\n",
      "num_nodes = 1 num_gpus_per_node = 4\n",
      "optimal_views.size = 262\n",
      "views.size() = 262\n",
      "###PEFT DEBUGGING### Operators reconstructed from optimized graph.\n",
      "###PEFT DEBUGGING### Starting inplace optimizations.\n",
      "###PEFT DEBUGGING### Mapping output tensors.\n",
      "ndim(1) dims[1 0 0 0]\n",
      "###PEFT DEBUGGING### Setting up NCCL communications.\n",
      "###PEFT DEBUGGING### compile_inference completed successfully.\n",
      "Loading weight file embed_tokens.weight\n",
      "Loading weight file layers.0.input_layernorm.weight\n",
      "Loading weight file layers.0.self_attn.q_proj.weight\n",
      "Loading weight file layers.0.self_attn.k_proj.weight\n",
      "Loading weight file layers.0.self_attn.v_proj.weight\n",
      "Loading weight file layers.0.self_attn.o_proj.weight\n",
      "Loading weight file layers.0.post_attention_layernorm.weight\n",
      "Loading weight file layers.0.mlp.gate_proj.weight\n",
      "Loading weight file layers.0.mlp.up_proj.weight\n",
      "Loading weight file layers.0.mlp.down_proj.weight\n",
      "Loading weight file layers.1.input_layernorm.weight\n",
      "Loading weight file layers.1.self_attn.q_proj.weight\n",
      "Loading weight file layers.1.self_attn.k_proj.weight\n",
      "Loading weight file layers.1.self_attn.v_proj.weight\n",
      "Loading weight file layers.1.self_attn.o_proj.weight\n",
      "Loading weight file layers.1.post_attention_layernorm.weight\n",
      "Loading weight file layers.1.mlp.gate_proj.weight\n",
      "Loading weight file layers.1.mlp.up_proj.weight\n",
      "Loading weight file layers.1.mlp.down_proj.weight\n",
      "Loading weight file layers.2.input_layernorm.weight\n",
      "Loading weight file layers.2.self_attn.q_proj.weight\n",
      "Loading weight file layers.2.self_attn.k_proj.weight\n",
      "Loading weight file layers.2.self_attn.v_proj.weight\n",
      "Loading weight file layers.2.self_attn.o_proj.weight\n",
      "Loading weight file layers.2.post_attention_layernorm.weight\n",
      "Loading weight file layers.2.mlp.gate_proj.weight\n",
      "Loading weight file layers.2.mlp.up_proj.weight\n",
      "Loading weight file layers.2.mlp.down_proj.weight\n",
      "Loading weight file layers.3.input_layernorm.weight\n",
      "Loading weight file layers.3.self_attn.q_proj.weight\n",
      "Loading weight file layers.3.self_attn.k_proj.weight\n",
      "Loading weight file layers.3.self_attn.v_proj.weight\n",
      "Loading weight file layers.3.self_attn.o_proj.weight\n",
      "Loading weight file layers.3.post_attention_layernorm.weight\n",
      "Loading weight file layers.3.mlp.gate_proj.weight\n",
      "Loading weight file layers.3.mlp.up_proj.weight\n",
      "Loading weight file layers.3.mlp.down_proj.weight\n",
      "Loading weight file layers.4.input_layernorm.weight\n",
      "Loading weight file layers.4.self_attn.q_proj.weight\n",
      "Loading weight file layers.4.self_attn.k_proj.weight\n",
      "Loading weight file layers.4.self_attn.v_proj.weight\n",
      "Loading weight file layers.4.self_attn.o_proj.weight\n",
      "Loading weight file layers.4.post_attention_layernorm.weight\n",
      "Loading weight file layers.4.mlp.gate_proj.weight\n",
      "Loading weight file layers.4.mlp.up_proj.weight\n",
      "Loading weight file layers.4.mlp.down_proj.weight\n",
      "Loading weight file layers.5.input_layernorm.weight\n",
      "Loading weight file layers.5.self_attn.q_proj.weight\n",
      "Loading weight file layers.5.self_attn.k_proj.weight\n",
      "Loading weight file layers.5.self_attn.v_proj.weight\n",
      "Loading weight file layers.5.self_attn.o_proj.weight\n",
      "Loading weight file layers.5.post_attention_layernorm.weight\n",
      "Loading weight file layers.5.mlp.gate_proj.weight\n",
      "Loading weight file layers.5.mlp.up_proj.weight\n",
      "Loading weight file layers.5.mlp.down_proj.weight\n",
      "Loading weight file layers.6.input_layernorm.weight\n",
      "Loading weight file layers.6.self_attn.q_proj.weight\n",
      "Loading weight file layers.6.self_attn.k_proj.weight\n",
      "Loading weight file layers.6.self_attn.v_proj.weight\n",
      "Loading weight file layers.6.self_attn.o_proj.weight\n",
      "Loading weight file layers.6.post_attention_layernorm.weight\n",
      "Loading weight file layers.6.mlp.gate_proj.weight\n",
      "Loading weight file layers.6.mlp.up_proj.weight\n",
      "Loading weight file layers.6.mlp.down_proj.weight\n",
      "Loading weight file layers.7.input_layernorm.weight\n",
      "Loading weight file layers.7.self_attn.q_proj.weight\n",
      "Loading weight file layers.7.self_attn.k_proj.weight\n",
      "Loading weight file layers.7.self_attn.v_proj.weight\n",
      "Loading weight file layers.7.self_attn.o_proj.weight\n",
      "Loading weight file layers.7.post_attention_layernorm.weight\n",
      "Loading weight file layers.7.mlp.gate_proj.weight\n",
      "Loading weight file layers.7.mlp.up_proj.weight\n",
      "Loading weight file layers.7.mlp.down_proj.weight\n",
      "Loading weight file layers.8.input_layernorm.weight\n",
      "Loading weight file layers.8.self_attn.q_proj.weight\n",
      "Loading weight file layers.8.self_attn.k_proj.weight\n",
      "Loading weight file layers.8.self_attn.v_proj.weight\n",
      "Loading weight file layers.8.self_attn.o_proj.weight\n",
      "Loading weight file layers.8.post_attention_layernorm.weight\n",
      "Loading weight file layers.8.mlp.gate_proj.weight\n",
      "Loading weight file layers.8.mlp.up_proj.weight\n",
      "Loading weight file layers.8.mlp.down_proj.weight\n",
      "Loading weight file layers.9.input_layernorm.weight\n",
      "Loading weight file layers.9.self_attn.q_proj.weight\n",
      "Loading weight file layers.9.self_attn.k_proj.weight\n",
      "Loading weight file layers.9.self_attn.v_proj.weight\n",
      "Loading weight file layers.9.self_attn.o_proj.weight\n",
      "Loading weight file layers.9.post_attention_layernorm.weight\n",
      "Loading weight file layers.9.mlp.gate_proj.weight\n",
      "Loading weight file layers.9.mlp.up_proj.weight\n",
      "Loading weight file layers.9.mlp.down_proj.weight\n",
      "Loading weight file layers.10.input_layernorm.weight\n",
      "Loading weight file layers.10.self_attn.q_proj.weight\n",
      "Loading weight file layers.10.self_attn.k_proj.weight\n",
      "Loading weight file layers.10.self_attn.v_proj.weight\n",
      "Loading weight file layers.10.self_attn.o_proj.weight\n",
      "Loading weight file layers.10.post_attention_layernorm.weight\n",
      "Loading weight file layers.10.mlp.gate_proj.weight\n",
      "Loading weight file layers.10.mlp.up_proj.weight\n",
      "Loading weight file layers.10.mlp.down_proj.weight\n",
      "Loading weight file layers.11.input_layernorm.weight\n",
      "Loading weight file layers.11.self_attn.q_proj.weight\n",
      "Loading weight file layers.11.self_attn.k_proj.weight\n",
      "Loading weight file layers.11.self_attn.v_proj.weight\n",
      "Loading weight file layers.11.self_attn.o_proj.weight\n",
      "Loading weight file layers.11.post_attention_layernorm.weight\n",
      "Loading weight file layers.11.mlp.gate_proj.weight\n",
      "Loading weight file layers.11.mlp.up_proj.weight\n",
      "Loading weight file layers.11.mlp.down_proj.weight\n",
      "Loading weight file layers.12.input_layernorm.weight\n",
      "Loading weight file layers.12.self_attn.q_proj.weight\n",
      "Loading weight file layers.12.self_attn.k_proj.weight\n",
      "Loading weight file layers.12.self_attn.v_proj.weight\n",
      "Loading weight file layers.12.self_attn.o_proj.weight\n",
      "Loading weight file layers.12.post_attention_layernorm.weight\n",
      "Loading weight file layers.12.mlp.gate_proj.weight\n",
      "Loading weight file layers.12.mlp.up_proj.weight\n",
      "Loading weight file layers.12.mlp.down_proj.weight\n",
      "Loading weight file layers.13.input_layernorm.weight\n",
      "Loading weight file layers.13.self_attn.q_proj.weight\n",
      "Loading weight file layers.13.self_attn.k_proj.weight\n",
      "Loading weight file layers.13.self_attn.v_proj.weight\n",
      "Loading weight file layers.13.self_attn.o_proj.weight\n",
      "Loading weight file layers.13.post_attention_layernorm.weight\n",
      "Loading weight file layers.13.mlp.gate_proj.weight\n",
      "Loading weight file layers.13.mlp.up_proj.weight\n",
      "Loading weight file layers.13.mlp.down_proj.weight\n",
      "Loading weight file layers.14.input_layernorm.weight\n",
      "Loading weight file layers.14.self_attn.q_proj.weight\n",
      "Loading weight file layers.14.self_attn.k_proj.weight\n",
      "Loading weight file layers.14.self_attn.v_proj.weight\n",
      "Loading weight file layers.14.self_attn.o_proj.weight\n",
      "Loading weight file layers.14.post_attention_layernorm.weight\n",
      "Loading weight file layers.14.mlp.gate_proj.weight\n",
      "Loading weight file layers.14.mlp.up_proj.weight\n",
      "Loading weight file layers.14.mlp.down_proj.weight\n",
      "Loading weight file layers.15.input_layernorm.weight\n",
      "Loading weight file layers.15.self_attn.q_proj.weight\n",
      "Loading weight file layers.15.self_attn.k_proj.weight\n",
      "Loading weight file layers.15.self_attn.v_proj.weight\n",
      "Loading weight file layers.15.self_attn.o_proj.weight\n",
      "Loading weight file layers.15.post_attention_layernorm.weight\n",
      "Loading weight file layers.15.mlp.gate_proj.weight\n",
      "Loading weight file layers.15.mlp.up_proj.weight\n",
      "Loading weight file layers.15.mlp.down_proj.weight\n",
      "Loading weight file layers.16.input_layernorm.weight\n",
      "Loading weight file layers.16.self_attn.q_proj.weight\n",
      "Loading weight file layers.16.self_attn.k_proj.weight\n",
      "Loading weight file layers.16.self_attn.v_proj.weight\n",
      "Loading weight file layers.16.self_attn.o_proj.weight\n",
      "Loading weight file layers.16.post_attention_layernorm.weight\n",
      "Loading weight file layers.16.mlp.gate_proj.weight\n",
      "Loading weight file layers.16.mlp.up_proj.weight\n",
      "Loading weight file layers.16.mlp.down_proj.weight\n",
      "Loading weight file layers.17.input_layernorm.weight\n",
      "Loading weight file layers.17.self_attn.q_proj.weight\n",
      "Loading weight file layers.17.self_attn.k_proj.weight\n",
      "Loading weight file layers.17.self_attn.v_proj.weight\n",
      "Loading weight file layers.17.self_attn.o_proj.weight\n",
      "Loading weight file layers.17.post_attention_layernorm.weight\n",
      "Loading weight file layers.17.mlp.gate_proj.weight\n",
      "Loading weight file layers.17.mlp.up_proj.weight\n",
      "Loading weight file layers.17.mlp.down_proj.weight\n",
      "Loading weight file layers.18.input_layernorm.weight\n",
      "Loading weight file layers.18.self_attn.q_proj.weight\n",
      "Loading weight file layers.18.self_attn.k_proj.weight\n",
      "Loading weight file layers.18.self_attn.v_proj.weight\n",
      "Loading weight file layers.18.self_attn.o_proj.weight\n",
      "Loading weight file layers.18.post_attention_layernorm.weight\n",
      "Loading weight file layers.18.mlp.gate_proj.weight\n",
      "Loading weight file layers.18.mlp.up_proj.weight\n",
      "Loading weight file layers.18.mlp.down_proj.weight\n",
      "Loading weight file layers.19.input_layernorm.weight\n",
      "Loading weight file layers.19.self_attn.q_proj.weight\n",
      "Loading weight file layers.19.self_attn.k_proj.weight\n",
      "Loading weight file layers.19.self_attn.v_proj.weight\n",
      "Loading weight file layers.19.self_attn.o_proj.weight\n",
      "Loading weight file layers.19.post_attention_layernorm.weight\n",
      "Loading weight file layers.19.mlp.gate_proj.weight\n",
      "Loading weight file layers.19.mlp.up_proj.weight\n",
      "Loading weight file layers.19.mlp.down_proj.weight\n",
      "Loading weight file layers.20.input_layernorm.weight\n",
      "Loading weight file layers.20.self_attn.q_proj.weight\n",
      "Loading weight file layers.20.self_attn.k_proj.weight\n",
      "Loading weight file layers.20.self_attn.v_proj.weight\n",
      "Loading weight file layers.20.self_attn.o_proj.weight\n",
      "Loading weight file layers.20.post_attention_layernorm.weight\n",
      "Loading weight file layers.20.mlp.gate_proj.weight\n",
      "Loading weight file layers.20.mlp.up_proj.weight\n",
      "Loading weight file layers.20.mlp.down_proj.weight\n",
      "Loading weight file layers.21.input_layernorm.weight\n",
      "Loading weight file layers.21.self_attn.q_proj.weight\n",
      "Loading weight file layers.21.self_attn.k_proj.weight\n",
      "Loading weight file layers.21.self_attn.v_proj.weight\n",
      "Loading weight file layers.21.self_attn.o_proj.weight\n",
      "Loading weight file layers.21.post_attention_layernorm.weight\n",
      "Loading weight file layers.21.mlp.gate_proj.weight\n",
      "Loading weight file layers.21.mlp.up_proj.weight\n",
      "Loading weight file layers.21.mlp.down_proj.weight\n",
      "Loading weight file layers.22.input_layernorm.weight\n",
      "Loading weight file layers.22.self_attn.q_proj.weight\n",
      "Loading weight file layers.22.self_attn.k_proj.weight\n",
      "Loading weight file layers.22.self_attn.v_proj.weight\n",
      "Loading weight file layers.22.self_attn.o_proj.weight\n",
      "Loading weight file layers.22.post_attention_layernorm.weight\n",
      "Loading weight file layers.22.mlp.gate_proj.weight\n",
      "Loading weight file layers.22.mlp.up_proj.weight\n",
      "Loading weight file layers.22.mlp.down_proj.weight\n",
      "Loading weight file layers.23.input_layernorm.weight\n",
      "Loading weight file layers.23.self_attn.q_proj.weight\n",
      "Loading weight file layers.23.self_attn.k_proj.weight\n",
      "Loading weight file layers.23.self_attn.v_proj.weight\n",
      "Loading weight file layers.23.self_attn.o_proj.weight\n",
      "Loading weight file layers.23.post_attention_layernorm.weight\n",
      "Loading weight file layers.23.mlp.gate_proj.weight\n",
      "Loading weight file layers.23.mlp.up_proj.weight\n",
      "Loading weight file layers.23.mlp.down_proj.weight\n",
      "Loading weight file layers.24.input_layernorm.weight\n",
      "Loading weight file layers.24.self_attn.q_proj.weight\n",
      "Loading weight file layers.24.self_attn.k_proj.weight\n",
      "Loading weight file layers.24.self_attn.v_proj.weight\n",
      "Loading weight file layers.24.self_attn.o_proj.weight\n",
      "Loading weight file layers.24.post_attention_layernorm.weight\n",
      "Loading weight file layers.24.mlp.gate_proj.weight\n",
      "Loading weight file layers.24.mlp.up_proj.weight\n",
      "Loading weight file layers.24.mlp.down_proj.weight\n",
      "Loading weight file layers.25.input_layernorm.weight\n",
      "Loading weight file layers.25.self_attn.q_proj.weight\n",
      "Loading weight file layers.25.self_attn.k_proj.weight\n",
      "Loading weight file layers.25.self_attn.v_proj.weight\n",
      "Loading weight file layers.25.self_attn.o_proj.weight\n",
      "Loading weight file layers.25.post_attention_layernorm.weight\n",
      "Loading weight file layers.25.mlp.gate_proj.weight\n",
      "Loading weight file layers.25.mlp.up_proj.weight\n",
      "Loading weight file layers.25.mlp.down_proj.weight\n",
      "Loading weight file layers.26.input_layernorm.weight\n",
      "Loading weight file layers.26.self_attn.q_proj.weight\n",
      "Loading weight file layers.26.self_attn.k_proj.weight\n",
      "Loading weight file layers.26.self_attn.v_proj.weight\n",
      "Loading weight file layers.26.self_attn.o_proj.weight\n",
      "Loading weight file layers.26.post_attention_layernorm.weight\n",
      "Loading weight file layers.26.mlp.gate_proj.weight\n",
      "Loading weight file layers.26.mlp.up_proj.weight\n",
      "Loading weight file layers.26.mlp.down_proj.weight\n",
      "Loading weight file layers.27.input_layernorm.weight\n",
      "Loading weight file layers.27.self_attn.q_proj.weight\n",
      "Loading weight file layers.27.self_attn.k_proj.weight\n",
      "Loading weight file layers.27.self_attn.v_proj.weight\n",
      "Loading weight file layers.27.self_attn.o_proj.weight\n",
      "Loading weight file layers.27.post_attention_layernorm.weight\n",
      "Loading weight file layers.27.mlp.gate_proj.weight\n",
      "Loading weight file layers.27.mlp.up_proj.weight\n",
      "Loading weight file layers.27.mlp.down_proj.weight\n",
      "Loading weight file layers.28.input_layernorm.weight\n",
      "Loading weight file layers.28.self_attn.q_proj.weight\n",
      "Loading weight file layers.28.self_attn.k_proj.weight\n",
      "Loading weight file layers.28.self_attn.v_proj.weight\n",
      "Loading weight file layers.28.self_attn.o_proj.weight\n",
      "Loading weight file layers.28.post_attention_layernorm.weight\n",
      "Loading weight file layers.28.mlp.gate_proj.weight\n",
      "Loading weight file layers.28.mlp.up_proj.weight\n",
      "Loading weight file layers.28.mlp.down_proj.weight\n",
      "Loading weight file layers.29.input_layernorm.weight\n",
      "Loading weight file layers.29.self_attn.q_proj.weight\n",
      "Loading weight file layers.29.self_attn.k_proj.weight\n",
      "Loading weight file layers.29.self_attn.v_proj.weight\n",
      "Loading weight file layers.29.self_attn.o_proj.weight\n",
      "Loading weight file layers.29.post_attention_layernorm.weight\n",
      "Loading weight file layers.29.mlp.gate_proj.weight\n",
      "Loading weight file layers.29.mlp.up_proj.weight\n",
      "Loading weight file layers.29.mlp.down_proj.weight\n",
      "Loading weight file layers.30.input_layernorm.weight\n",
      "Loading weight file layers.30.self_attn.q_proj.weight\n",
      "Loading weight file layers.30.self_attn.k_proj.weight\n",
      "Loading weight file layers.30.self_attn.v_proj.weight\n",
      "Loading weight file layers.30.self_attn.o_proj.weight\n",
      "Loading weight file layers.30.post_attention_layernorm.weight\n",
      "Loading weight file layers.30.mlp.gate_proj.weight\n",
      "Loading weight file layers.30.mlp.up_proj.weight\n",
      "Loading weight file layers.30.mlp.down_proj.weight\n",
      "Loading weight file layers.31.input_layernorm.weight\n",
      "Loading weight file layers.31.self_attn.q_proj.weight\n",
      "Loading weight file layers.31.self_attn.k_proj.weight\n",
      "Loading weight file layers.31.self_attn.v_proj.weight\n",
      "Loading weight file layers.31.self_attn.o_proj.weight\n",
      "Loading weight file layers.31.post_attention_layernorm.weight\n",
      "Loading weight file layers.31.mlp.gate_proj.weight\n",
      "Loading weight file layers.31.mlp.up_proj.weight\n",
      "Loading weight file layers.31.mlp.down_proj.weight\n",
      "Loading weight file norm.weight\n",
      "Loading weight file lm_head.weight\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "[0 - 7fec10098740]   22.927450 {3}{RequestManager}: Output token is: 128001\n",
      "[0 - 7fec10098740]   22.928224 {3}{RequestManager}: [Done] guid(1000001) final_length(35)\n",
      "[0 - 7fec10098740]   22.928235 {3}{RequestManager}: Final output: <s> <|begin_of_text|>Why can camels survive for long without water? Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.<|end_of_text|>\n",
      "[0 - 7fec10098740]   22.928241 {3}{RequestManager}: [Profile] guid(1000001) llm_decoding_steps(1) start(22404764.0) finish(22928237.0) latency(523473.0) ttft(21574986.0)\n",
      "[0 - 7fec1008c740]   23.009957 {3}{RequestManager}: [Finetuning] guid(1000000) completed_training_steps(2) processed_finetuning_tokens(65) latency(23009919.0)\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7fec2ce83b50>]\n",
      "[0]10445\n",
      "[1]649\n",
      "[2]6730\n",
      "[3]2053\n",
      "[4]18167\n",
      "[5]369\n",
      "[6]1317\n",
      "[7]2085\n",
      "[8]3090\n",
      "[9]30\n",
      "[10]8215\n",
      "[11]2053\n",
      "[12]1005\n",
      "[13]279\n",
      "[14]8834\n",
      "[15]304\n",
      "[16]872\n",
      "[17]305\n",
      "[18]12055\n",
      "[19]311\n",
      "[20]2567\n",
      "[21]1124\n",
      "[22]10409\n",
      "[23]449\n",
      "[24]4907\n",
      "[25]323\n",
      "[26]88000\n",
      "[27]369\n",
      "[28]1317\n",
      "[29]18852\n",
      "[30]315\n",
      "[31]892\n",
      "[32]13\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7fec72884280]   23.012000 {3}{RequestManager}: [1000002]New request tokens: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30 8215 2053 1005 279 8834 304 872 305 12055 311 2567 1124 10409 449 4907 323 88000 369 1317 18852 315 892 13\n",
      "[0 - 7fec10098740]   23.217673 {3}{RequestManager}: Output token is: 128001\n",
      "[0 - 7fec10098740]   23.217763 {3}{RequestManager}: [Done] guid(1000002) final_length(35)\n",
      "[0 - 7fec10098740]   23.217770 {3}{RequestManager}: Final output: <s> <|begin_of_text|>Why can camels survive for long without water? Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.<|end_of_text|>\n",
      "[0 - 7fec10098740]   23.217779 {3}{RequestManager}: [Profile] guid(1000002) llm_decoding_steps(1) start(23102299.0) finish(23217773.0) latency(115474.0) ttft(205646.0)\n",
      "2024-07-21 02:29:47 - ###PEFT DEBUGGING### Background serving task completed.\n",
      "Background server stopped.\n"
     ]
    }
   ],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"inference_dataset\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "download_models_used(base_model=configs_dict[\"base_model\"],\n",
    "                     peft_model=configs_dict[\"finetuning_peft_model_id\"])\n",
    "\n",
    "demo = FlexFlowDemo(configs_dict)\n",
    "\n",
    "demo.initialize_flexflow()\n",
    "demo.start_server()\n",
    "demo.generate_finetuning()\n",
    "demo.generate_inference()\n",
    "demo.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
