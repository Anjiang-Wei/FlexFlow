{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlexFlow Co-Serving Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, subprocess, os\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from huggingface_hub import HfFolder\n",
    "import flexflow.serve as ff\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(finetune_dataset_size=2, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path -- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    inference_data = []\n",
    "    finetuning_data = []\n",
    "    for row in dataset:\n",
    "        if len(finetuning_data) == finetune_dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            inference_data.append(row['instruction'])\n",
    "            finetuning_data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(inference_data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(finetuning_data[:1], file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 4,\n",
    "    \"memory_per_gpu\": 14000,\n",
    "    \"zero_copy_memory_per_node\": 40000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 4,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 10,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"finetuning_peft_model_id\": \"flechman/llama-3-8b-lora-dolly\",\n",
    "    \"cache_path\": os.environ.get(\"FF_CACHE_PATH\", \"\"),\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"finetuning_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "configs = SimpleNamespace(**configs_dict)\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"inference_dataset\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "# Clear output file\n",
    "with open(configs.output_file, 'w') as file:\n",
    "    file.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download base and peft inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [configs.inference_peft_model_id, '--base_model_name', configs.base_model]\n",
    "hf_token = input(\"Please enter your HuggingFace personal access token: \")\n",
    "subprocess.run(['huggingface-cli', 'login', '--token', hf_token])\n",
    "subprocess.run(['python', '../../utils/download_peft_model.py'] + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize FlexFlow runtime and LLM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FlexFlow runtime. ff.init() takes a dictionary or the path to a JSON file with the configs\n",
    "ff.init(configs_dict)\n",
    "\n",
    "# Create the FlexFlow LLM\n",
    "ff_data_type = (\n",
    "    ff.DataType.DT_FLOAT if configs.full_precision else ff.DataType.DT_HALF\n",
    ")\n",
    "llm = ff.LLM(\n",
    "    configs.base_model,\n",
    "    data_type=ff_data_type,\n",
    "    cache_path=configs.cache_path,\n",
    "    refresh_cache=configs.refresh_cache,\n",
    "    output_file=configs.output_file,\n",
    ")\n",
    "# Add inference and/or finetuning lora\n",
    "lora_inference_config = None\n",
    "lora_finetuning_config = None\n",
    "if len(configs.inference_dataset) > 0:\n",
    "    lora_inference_config = ff.LoraLinearConfig(\n",
    "        llm.cache_path, \n",
    "        configs.inference_peft_model_id,\n",
    "        base_model_name_or_path=configs.base_model\n",
    "    )\n",
    "    llm.add_peft(lora_inference_config)\n",
    "if len(configs.finetuning_dataset) > 0:\n",
    "    lora_finetuning_config = ff.LoraLinearConfig(\n",
    "        llm.cache_path,\n",
    "        configs.finetuning_peft_model_id,\n",
    "        trainable=True,\n",
    "        init_lora_weights=True,\n",
    "        rank=16,\n",
    "        lora_alpha=16.0,\n",
    "        target_modules = [\"down_proj\"],\n",
    "        base_model_name_or_path=configs.base_model,\n",
    "        optimizer_type=ff.OptimizerType.OPTIMIZER_TYPE_SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"learning_rate\": configs.learning_rate,\n",
    "            \"momentum\": configs.momentum,\n",
    "            \"weight_decay\": configs.weight_decay,\n",
    "            \"nesterov\": configs.nesterov,\n",
    "        },\n",
    "    )\n",
    "    llm.add_peft(lora_finetuning_config)\n",
    "\n",
    "# Compile the LLM for inference and load the weights into memory\n",
    "generation_config = ff.GenerationConfig(\n",
    "    do_sample=configs.do_sample,\n",
    "    temperature=configs.temperature,\n",
    "    topp=configs.topp,\n",
    "    topk=configs.topk\n",
    ")\n",
    "enable_peft_finetuning = len(configs.finetuning_dataset) > 0\n",
    "llm.compile(\n",
    "    generation_config,\n",
    "    enable_peft_finetuning=enable_peft_finetuning,\n",
    "    max_requests_per_batch=configs.max_requests_per_batch+int(enable_peft_finetuning),\n",
    "    max_seq_length=configs.max_sequence_length,\n",
    "    max_tokens_per_batch=configs.max_tokens_per_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the LLM Co-serving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.start_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [s for s in json.load(open(configs.inference_dataset))]\n",
    "inference_requests = [\n",
    "    ff.Request(\n",
    "        ff.RequestType.REQ_INFERENCE,\n",
    "        prompt=prompt,\n",
    "        max_sequence_length=configs.max_sequence_length,\n",
    "        peft_model_id=llm.get_ff_peft_id(lora_inference_config),\n",
    "    )\n",
    "    for prompt in prompts\n",
    "]\n",
    "inf_req_res_1 = llm.generate(inference_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Finetuning on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_request = ff.Request(\n",
    "    ff.RequestType.REQ_FINETUNING,\n",
    "    max_sequence_length=configs.max_sequence_length,\n",
    "    peft_model_id=llm.get_ff_peft_id(lora_finetuning_config),\n",
    "    dataset_filepath=os.path.join(os.getcwd(), configs.finetuning_dataset),\n",
    "    max_training_steps=configs.max_training_steps,\n",
    ")\n",
    "ft_res = llm.generate([finetuning_request])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save finetuned model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = input(\"Please enter your HuggingFace personal access token: \")\n",
    "subprocess.run(['huggingface-cli', 'login', '--token', hf_token])\n",
    "subprocess.run(['python', '../../utils/upload_peft_model.py'] + [configs.finetuning_peft_model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download finetuned model from HuggingFace and use it for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_inference_config = ff.LoraLinearConfig(\n",
    "    llm.cache_path, \n",
    "    configs.finetuning_peft_model_id,\n",
    "    base_model_name_or_path=configs.base_model\n",
    ")\n",
    "llm.add_peft(lora_inference_config)\n",
    "\n",
    "args = [configs.finetuning_peft_model_id, '--base_model_name', configs.base_model]\n",
    "#hf_token = input(\"Please enter your HuggingFace personal access token: \")\n",
    "subprocess.run(['huggingface-cli', 'login', '--token', hf_token])\n",
    "subprocess.run(['python', '../../utils/download_peft_model.py'] + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inference (using finetuned model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [s for s in json.load(open(configs.inference_dataset))]\n",
    "inference_requests = [\n",
    "    ff.Request(\n",
    "        ff.RequestType.REQ_INFERENCE,\n",
    "        prompt=prompt,\n",
    "        max_sequence_length=configs.max_sequence_length,\n",
    "        peft_model_id=llm.get_ff_peft_id(lora_inference_config),\n",
    "    )\n",
    "    for prompt in prompts\n",
    "]\n",
    "inf_req_res_2 = llm.generate(inference_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop LLM Co-serving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==Inference result before finetuning: \", inf_req_res_1[0].output_text)\n",
    "print(\"==Inference result after finetuning: \", inf_req_res_2[0].output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(configs_dict[\"max_training_steps\"]))\n",
    "loss_values = ft_res[0].finetuning_losses\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.show(epochs, loss_values, marker='o', linestyle='-', color='b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
