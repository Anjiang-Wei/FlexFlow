{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from datasets import load_dataset\n",
    "from demo_class import FlexFlowDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_size=10, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path-- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if len(data) == dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 8192,\n",
    "    \"zero_copy_memory_per_node\": 12000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 10,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"JackFram/llama-160m\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-160m-lora\",\n",
    "    \"cache_path\": \"\",\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": True,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"finetuning_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 7f3c2ae3f280]    0.350369 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f3c2ae3f280]    0.350608 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f3c2ae3f280]    0.350676 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f3c2ae3f280]    0.350741 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f3c2ae3f280]    0.350804 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[0 - 7f3c2ae3f280]    0.477395 {3}{flexflow_c}: [FFConfig] new 0x75fc8d0\n",
      "[0 - 7f3c2ae3f280]    0.477563 {3}{flexflow_c}: [RequestManager] get 0xa292510\n",
      "[0 - 7f3c2ae3f280]    0.477610 {3}{flexflow_c}: [RequestManager] set max_requests_per_batch 1\n",
      "[0 - 7f3c2ae3f280]    0.477663 {3}{flexflow_c}: [RequestManager] set max_tokens_per_batch 128\n",
      "[0 - 7f3c2ae3f280]    0.477684 {3}{flexflow_c}: [RequestManager] set max_sequence_length 256\n",
      "[0 - 7f3c2ae3f280]    0.477725 {3}{flexflow_c}: [RequestManager] set_enable_peft_finetuning 1\n",
      "workSpaceSize (128 MB)\n",
      "[0 - 7f3c2ae3f280]    0.650248 {3}{flexflow_c}: [FFModel] new 0xa2a0160\n",
      "[0 - 7f3c2ae3f280]    0.678214 {3}{flexflow_c}: [Tensor] new 2D 0xa298cd0 (1, 128, 0, 0)\n",
      "[0 - 7f3c2ae3f280]    0.704600 {3}{flexflow_c}: [Tensor] get dims [0, 0, 128, 1]\n",
      "[0 - 7f3c2ae3f280]    0.704729 {3}{flexflow_c}: [UniformInitializer] new 0x9f99490\n",
      "[0 - 7f3c2ae3f280]    0.809203 {3}{flexflow_c}: [Embedding] new Tensor 0x33bdae0, input 0xa298cd0, num_entries 32000, out_dim 768, aggr 20, dtype 44, shared_op (nil), kernel_init 0x9f99490, name embed_tokens\n",
      "[0 - 7f3c2ae3f280]    0.809739 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.810286 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.810519 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.810684 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.810715 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.810923 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed6450 (3072, 1, 128, 0), input 0x9ee73f0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.810950 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811039 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed6340 (3072, 1, 128, 0), input 0x9ee73f0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.811063 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811164 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0x9ed67d0, input1 0x9ed6450, input2 0x9ed6340, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.811201 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811288 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa296fd0 (768, 1, 128, 0), input 0x9ed67d0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.0.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.811309 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811394 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811426 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811523 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811603 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811635 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.811716 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa29abb0 (3072, 1, 128, 0), input 0xa29a8d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.811734 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811812 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa29aaa0 (3072, 1, 128, 0), input 0xa29a8d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.811829 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811880 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa29af30, input1 0xa29abb0, input2 0xa29aaa0, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.811900 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.811981 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa29b3c0 (768, 1, 128, 0), input 0xa29af30, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.1.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.811998 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812080 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812112 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812202 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812279 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812309 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812381 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa2dc2d0 (3072, 1, 128, 0), input 0xa2dc330, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.812399 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.812472 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa2dc500 (3072, 1, 128, 0), input 0xa2dc330, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.812490 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.812540 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa2dc940, input1 0xa2dc2d0, input2 0xa2dc500, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.812561 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.812643 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa2dcdd0 (768, 1, 128, 0), input 0xa2dc940, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.2.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.812660 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812736 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812769 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812865 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812941 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.812971 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813048 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa2dde80 (3072, 1, 128, 0), input 0xa2ddee0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.813065 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813142 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa2de0b0 (3072, 1, 128, 0), input 0xa2ddee0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.813158 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813204 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa2de4f0, input1 0xa2dde80, input2 0xa2de0b0, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.813224 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813304 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed6ed0 (768, 1, 128, 0), input 0xa2de4f0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.3.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.813321 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813398 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813434 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813526 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813604 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813627 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.813709 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed7b20 (3072, 1, 128, 0), input 0x9ed7b80, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.813727 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813800 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed7d50 (3072, 1, 128, 0), input 0x9ed7b80, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.813817 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813864 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0x9ed8190, input1 0x9ed7b20, input2 0x9ed7d50, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.813883 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.813963 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed8620 (768, 1, 128, 0), input 0x9ed8190, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.4.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.813980 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814048 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814079 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814166 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814243 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814272 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814344 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed9270 (3072, 1, 128, 0), input 0x9ed92d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.814359 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.814434 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed94a0 (3072, 1, 128, 0), input 0x9ed92d0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.814454 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.814505 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0x9ed98e0, input1 0x9ed9270, input2 0x9ed94a0, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.814524 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.814606 {3}{flexflow_c}: [Dense] new Tensor 2D 0x9ed9d70 (768, 1, 128, 0), input 0x9ed98e0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.5.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.814631 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814700 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814730 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814822 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814897 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.814920 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815001 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa503250 (3072, 1, 128, 0), input 0xa50f450, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.815018 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815093 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa50f620 (3072, 1, 128, 0), input 0xa50f450, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.815110 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815150 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa51bc00, input1 0xa503250, input2 0xa50f620, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.815169 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815244 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa540570 (768, 1, 128, 0), input 0xa51bc00, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.6.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.815260 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815335 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815358 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815461 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815528 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815557 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815635 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa559500 (3072, 1, 128, 0), input 0xa565700, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.815652 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815721 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa5658d0 (3072, 1, 128, 0), input 0xa565700, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.815737 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815784 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa571eb0, input1 0xa559500, input2 0xa5658d0, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.815803 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.815883 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa596820 (768, 1, 128, 0), input 0xa571eb0, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.7.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.815899 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815973 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.815994 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816086 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816151 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816172 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816254 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa5af7b0 (3072, 1, 128, 0), input 0xa5bb9e0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.816271 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.816334 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa5bbbb0 (3072, 1, 128, 0), input 0xa5bb9e0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.816349 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.816397 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa5c8190, input1 0xa5af7b0, input2 0xa5bbbb0, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.816421 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.816493 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa5ecb00 (768, 1, 128, 0), input 0xa5c8190, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.8.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.816510 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816580 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816601 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816698 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816772 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816801 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.816876 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa605a90 (3072, 1, 128, 0), input 0xa611cc0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.816893 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.816966 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa611e90 (3072, 1, 128, 0), input 0xa611cc0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.816982 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.817025 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa61e470, input1 0xa605a90, input2 0xa611e90, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.817044 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.817121 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa642de0 (768, 1, 128, 0), input 0xa61e470, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.9.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.817136 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817212 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817242 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817327 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817389 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817423 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817500 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa65bd70 (3072, 1, 128, 0), input 0xa667fa0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.817516 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.817588 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa668170 (3072, 1, 128, 0), input 0xa667fa0, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.817604 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.817642 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa674750, input1 0xa65bd70, input2 0xa668170, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.817660 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.817733 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa6990c0 (768, 1, 128, 0), input 0xa674750, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.10.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.817749 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817815 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817844 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.817931 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818006 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818035 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818106 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa6b2050 (3072, 1, 128, 0), input 0xa6be280, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.gate_proj\n",
      "[0 - 7f3c2ae3f280]    0.818122 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.818193 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa6be450 (3072, 1, 128, 0), input 0xa6be280, out_dim 3072, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.up_proj\n",
      "[0 - 7f3c2ae3f280]    0.818208 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.818257 {3}{flexflow_c}: [SigmoidSiluMulti] new Tensor 0xa6caa30, input1 0xa6b2050, input2 0xa6be450, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.818276 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 3072]\n",
      "[0 - 7f3c2ae3f280]    0.818346 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa6ef3a0 (768, 1, 128, 0), input 0xa6caa30, out_dim 768, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name layers.11.mlp.down_proj\n",
      "[0 - 7f3c2ae3f280]    0.818362 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818434 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818466 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 768]\n",
      "[0 - 7f3c2ae3f280]    0.818546 {3}{flexflow_c}: [Dense] new Tensor 2D 0xa4ea250 (32000, 1, 128, 0), input 0xa6fb880, out_dim 32000, activation 10, use_bias 0, shared_op (nil), kernel_init (nil), bias_init (nil), name lm_head\n",
      "[0 - 7f3c2ae3f280]    0.818562 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 32000]\n",
      "[0 - 7f3c2ae3f280]    0.818673 {3}{flexflow_c}: [Softmax] new Tensor 0xa4ea5d0, input 0xa4ea250, name (null)\n",
      "[0 - 7f3c2ae3f280]    0.818710 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 32000]\n",
      "[0 - 7f3c2ae3f280]    0.818837 {3}{flexflow_c}: [Tensor] get dims [0, 128, 1, 1]\n",
      "[0 - 7f3c2ae3f280]    0.819509 {3}{flexflow_c}: [LoraLinearConfig] new 0xa229080\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n",
      "[0 - 7f3c2ae3f280]    0.819992 {3}{flexflow_c}: [Add Lora Layer] model handle: 0xa2a0160, peft_config handle 0xa229080, peft_model_id: 0x9fa1d20\n",
      "[0 - 7f3c2ae3f280]    0.820194 {3}{flexflow_c}: [LoraLinearConfig] new 0xa1a4f30\n",
      "[0 - 7f3c2ae3f280]    0.822164 {3}{flexflow_c}: [Add Lora Layer] model handle: 0xa2a0160, peft_config handle 0xa1a4f30, peft_model_id: 0xa11baa0\n",
      "[0 - 7f3c2ae3f280]    0.964997 {3}{flexflow_c}: [RequestManager] set max_spec_tree_token_num 20\n",
      "[0 - 7f3c2ae3f280]    0.965103 {3}{flexflow_c}: [FileDataLoader] new 0xa722040\n",
      "[0 - 7f3c2ae3f280]    0.965184 {3}{flexflow_c}: [InferenceManager] get 0xa721c30\n",
      "[0 - 7f3c2ae3f280]    0.965224 {3}{flexflow_c}: [InferenceManager] register_model_weights_loader 0xa721c30 0xa2a0160 0xa722040\n",
      "Loading tokenizer...\n",
      "[0 - 7f3c2ae3f280]    1.027809 {3}{flexflow_c}: [RequestManager] register tokenizer 0xa292510 /root/.cache/flexflow/tokenizers/jackfram/llama-160m\n",
      "[0 - 7f3c2ae3f280]    1.027886 {3}{flexflow_c}: [RequestManager] register output filepath 0xa292510 peft_demo.txt\n",
      "[0 - 7f3c2ae3f280]    1.028145 {3}{flexflow_c}: [RequestManager] start background server 0xa292510 0xa2a0160\n",
      "Background server started.\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7f3bc7038f90>]\n",
      "[0 - 7f3c2ae3f280]    1.029062 {3}{flexflow_c}: [Model] generate[0] 0xa2a0160 /usr/FlexFlow/inference/python/peft_demo/finetuning_dataset.json 256 10\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7f3c2ae3f280]    1.030232 {3}{RequestManager}: [0] input: 1 3750 508 3949 1379 10503 573 363 1472 1728 4094 29973 5500 1379 671 278 9950 297 1009 3165 567 304 3013 963 10423 411 5864 322 27246 29878 362 363 1472 23704 310 931 29889\n",
      "[0 - 7f3c2ae3f280]    1.030238 {3}{RequestManager}: [0] output:\n",
      "[0 - 7f3c2ae3f280]    1.030251 {3}{RequestManager}: [1] input: 1 16308 29915 29879 11825 505 2211 29215 29901 28533 29892 23010 29891 29892 322 825 30010 29879 278 1024 310 278 4654 8750 29973 450 1024 310 278 4654 8750 338 16308\n",
      "[0 - 7f3c2ae3f280]    1.030253 {3}{RequestManager}: [1] output:\n",
      "[0 - 7f3c2ae3f280]    1.030260 {3}{RequestManager}: [2] input: 1 11644 4846 278 8291 278 2982 297 23526 304 2048 1009 379 29984 2259 360 1528 4937 29888 4539\n",
      "[0 - 7f3c2ae3f280]    1.030262 {3}{RequestManager}: [2] output:\n",
      "[0 - 7f3c2ae3f280]    1.030303 {3}{RequestManager}: [3] input: 1 1724 338 263 29807 29973 319 29807 338 263 883 297 1879 7843 29889 29871 739 338 263 2323 22112 10694 1754 310 16791 3454 322 738 1353 310 13791 29889 29871 739 338 263 5764 9704 310 6631 1196 24611 470 12770 29889 29871 450 13791 310 278 29807 526 8429 988 1023 12770 5870 29889 29871 1222 9422 310 1248 4790 787 526 15090 351 787 29892 11137 351 787 29892 322 4725 351 787 29889 29871 3139 10694 393 947 451 1712 12770 470 13791 338 451 263 29807 29889 29871 530 1342 310 263 1661 29899 3733 17125 338 263 8607 29889\n",
      "[0 - 7f3c2ae3f280]    1.030306 {3}{RequestManager}: [3] output:\n",
      "[0 - 7f3c2ae3f280]    1.030319 {3}{RequestManager}: [4] input: 1 8449 23238 310 4259 3023 310 8448 310 498 1617 267 1258 3375 1808 4326 29931 8326 1513 29973 2296 10624 376 29949 493 23935 29908 322 376 6730 310 3600 4408 29908 278 11582 322 18615 23238 310 4259 3023 29892 8307 29889\n",
      "[0 - 7f3c2ae3f280]    1.030322 {3}{RequestManager}: [4] output:\n",
      "[0 - 7f3c2ae3f280]    1.030333 {3}{RequestManager}: [5] input: 1 1724 5375 756 2113 278 1556 19025 7684 1612 1338 297 278 4955 310 278 8090 29973 5765 1963 295 567 756 2113 278 1556 7684 1612 1338 310 599 931 411 29871 29906 29941 330 3361 29889\n",
      "[0 - 7f3c2ae3f280]    1.030336 {3}{RequestManager}: [5] output:\n",
      "[0 - 7f3c2ae3f280]    1.030342 {3}{RequestManager}: [6] input: 1 8449 14872 7664 23139 1346 29954 5168 411 263 21265 29880 382 23693 30024 29973 478 10877 261\n",
      "[0 - 7f3c2ae3f280]    1.030344 {3}{RequestManager}: [6] output:\n",
      "[0 - 7f3c2ae3f280]    1.030350 {3}{RequestManager}: [7] input: 1 1724 5930 746 278 6575 5771 1623 29973 1932 278 6575 6166 29892 278 11005 8665 29889\n",
      "[0 - 7f3c2ae3f280]    1.030352 {3}{RequestManager}: [7] output:\n",
      "[0 - 7f3c2ae3f280]    1.030382 {3}{RequestManager}: [8] input: 1 1724 338 263 9750 29973 319 9750 338 385 3158 1734 393 16612 385 6354 29889 29871 1222 9422 310 1147 5824 526 278 1494 29901 2381 25217 29892 298 638 292 29892 289 638 292 29892 9679 261 2071 1218 29892 470 2071 2941 4357 29889 29871 2178 310 1438 2323 322 10296 1734 6455 526 21351 304 385 6354 393 738 8471 2655 508 437 29889 29871 1152 1342 29892 263 11203 508 4768 446 22203 411 263 5199 746 278 5199 338 8939 12818 278 4768 446 29889 29871 26646 671 338 451 9078 304 25618 470 2305 871 541 16058 304 599 8471 2712 29889\n",
      "[0 - 7f3c2ae3f280]    1.030385 {3}{RequestManager}: [8] output:\n",
      "[0 - 7f3c2ae3f280]    1.030401 {3}{RequestManager}: [9] input: 1 11644 3897 6989 310 20262 297 29871 29896 29947 29900 29953 29973 4667 306 310 278 24553 3897 6989 310 20262 297 29871 29896 29947 29900 29953 29889\n",
      "[0 - 7f3c2ae3f280]    1.030403 {3}{RequestManager}: [9] output:\n",
      "2024-07-17 15:41:30 - ###PEFT DEBUGGING### Starting background serving task.\n",
      "2024-07-17 15:41:30 - ###PEFT DEBUGGING### Updated models' configuration.\n",
      "###PEFT DEBUGGING### LLM Model object exists.\n",
      "###PEFT DEBUGGING### Model object exists.\n",
      "###PEFT DEBUGGING### Model object still exists.\n",
      "###PEFT DEBUGGING### Entering compile_inference.\n",
      "###PEFT DEBUGGING### Configuration check passed: At least four CPU cores per node.\n",
      "###PEFT DEBUGGING### Launching graph optimization task.\n",
      "num_nodes = 1 num_gpus_per_node = 1\n",
      "optimal_views.size = 102\n",
      "views.size() = 102\n",
      "###PEFT DEBUGGING### Operators reconstructed from optimized graph.\n",
      "###PEFT DEBUGGING### Starting inplace optimizations.\n",
      "###PEFT DEBUGGING### Mapping output tensors.\n",
      "ndim(1) dims[1 0 0 0]\n",
      "###PEFT DEBUGGING### Setting up NCCL communications.\n",
      "###PEFT DEBUGGING### compile_inference completed successfully.\n",
      "Loading weight file embed_tokens.weight\n",
      "Loading weight file layers.0.input_layernorm.weight\n",
      "Loading weight file layers.0.self_attn.q_proj.weight\n",
      "Loading weight file layers.0.self_attn.k_proj.weight\n",
      "Loading weight file layers.0.self_attn.v_proj.weight\n",
      "Loading weight file layers.0.self_attn.o_proj.weight\n",
      "Loading weight file layers.0.post_attention_layernorm.weight\n",
      "Loading weight file layers.0.mlp.gate_proj.weight\n",
      "Loading weight file layers.0.mlp.up_proj.weight\n",
      "Loading weight file layers.0.mlp.down_proj.weight\n",
      "Loading weight file layers.1.input_layernorm.weight\n",
      "Loading weight file layers.1.self_attn.q_proj.weight\n",
      "Loading weight file layers.1.self_attn.k_proj.weight\n",
      "Loading weight file layers.1.self_attn.v_proj.weight\n",
      "Loading weight file layers.1.self_attn.o_proj.weight\n",
      "Loading weight file layers.1.post_attention_layernorm.weight\n",
      "Loading weight file layers.1.mlp.gate_proj.weight\n",
      "Loading weight file layers.1.mlp.up_proj.weight\n",
      "Loading weight file layers.1.mlp.down_proj.weight\n",
      "Loading weight file layers.2.input_layernorm.weight\n",
      "Loading weight file layers.2.self_attn.q_proj.weight\n",
      "Loading weight file layers.2.self_attn.k_proj.weight\n",
      "Loading weight file layers.2.self_attn.v_proj.weight\n",
      "Loading weight file layers.2.self_attn.o_proj.weight\n",
      "Loading weight file layers.2.post_attention_layernorm.weight\n",
      "Loading weight file layers.2.mlp.gate_proj.weight\n",
      "Loading weight file layers.2.mlp.up_proj.weight\n",
      "Loading weight file layers.2.mlp.down_proj.weight\n",
      "Loading weight file layers.3.input_layernorm.weight\n",
      "Loading weight file layers.3.self_attn.q_proj.weight\n",
      "Loading weight file layers.3.self_attn.k_proj.weight\n",
      "Loading weight file layers.3.self_attn.v_proj.weight\n",
      "Loading weight file layers.3.self_attn.o_proj.weight\n",
      "Loading weight file layers.3.post_attention_layernorm.weight\n",
      "Loading weight file layers.3.mlp.gate_proj.weight\n",
      "Loading weight file layers.3.mlp.up_proj.weight\n",
      "Loading weight file layers.3.mlp.down_proj.weight\n",
      "Loading weight file layers.4.input_layernorm.weight\n",
      "Loading weight file layers.4.self_attn.q_proj.weight\n",
      "Loading weight file layers.4.self_attn.k_proj.weight\n",
      "Loading weight file layers.4.self_attn.v_proj.weight\n",
      "Loading weight file layers.4.self_attn.o_proj.weight\n",
      "Loading weight file layers.4.post_attention_layernorm.weight\n",
      "Loading weight file layers.4.mlp.gate_proj.weight\n",
      "Loading weight file layers.4.mlp.up_proj.weight\n",
      "Loading weight file layers.4.mlp.down_proj.weight\n",
      "Loading weight file layers.5.input_layernorm.weight\n",
      "Loading weight file layers.5.self_attn.q_proj.weight\n",
      "Loading weight file layers.5.self_attn.k_proj.weight\n",
      "Loading weight file layers.5.self_attn.v_proj.weight\n",
      "Loading weight file layers.5.self_attn.o_proj.weight\n",
      "Loading weight file layers.5.post_attention_layernorm.weight\n",
      "Loading weight file layers.5.mlp.gate_proj.weight\n",
      "Loading weight file layers.5.mlp.up_proj.weight\n",
      "Loading weight file layers.5.mlp.down_proj.weight\n",
      "Loading weight file layers.6.input_layernorm.weight\n",
      "Loading weight file layers.6.self_attn.q_proj.weight\n",
      "Loading weight file layers.6.self_attn.k_proj.weight\n",
      "Loading weight file layers.6.self_attn.v_proj.weight\n",
      "Loading weight file layers.6.self_attn.o_proj.weight\n",
      "Loading weight file layers.6.post_attention_layernorm.weight\n",
      "Loading weight file layers.6.mlp.gate_proj.weight\n",
      "Loading weight file layers.6.mlp.up_proj.weight\n",
      "Loading weight file layers.6.mlp.down_proj.weight\n",
      "Loading weight file layers.7.input_layernorm.weight\n",
      "Loading weight file layers.7.self_attn.q_proj.weight\n",
      "Loading weight file layers.7.self_attn.k_proj.weight\n",
      "Loading weight file layers.7.self_attn.v_proj.weight\n",
      "Loading weight file layers.7.self_attn.o_proj.weight\n",
      "Loading weight file layers.7.post_attention_layernorm.weight\n",
      "Loading weight file layers.7.mlp.gate_proj.weight\n",
      "Loading weight file layers.7.mlp.up_proj.weight\n",
      "Loading weight file layers.7.mlp.down_proj.weight\n",
      "Loading weight file layers.8.input_layernorm.weight\n",
      "Loading weight file layers.8.self_attn.q_proj.weight\n",
      "Loading weight file layers.8.self_attn.k_proj.weight\n",
      "Loading weight file layers.8.self_attn.v_proj.weight\n",
      "Loading weight file layers.8.self_attn.o_proj.weight\n",
      "Loading weight file layers.8.post_attention_layernorm.weight\n",
      "Loading weight file layers.8.mlp.gate_proj.weight\n",
      "Loading weight file layers.8.mlp.up_proj.weight\n",
      "Loading weight file layers.8.mlp.down_proj.weight\n",
      "Loading weight file layers.9.input_layernorm.weight\n",
      "Loading weight file layers.9.self_attn.q_proj.weight\n",
      "Loading weight file layers.9.self_attn.k_proj.weight\n",
      "Loading weight file layers.9.self_attn.v_proj.weight\n",
      "Loading weight file layers.9.self_attn.o_proj.weight\n",
      "Loading weight file layers.9.post_attention_layernorm.weight\n",
      "Loading weight file layers.9.mlp.gate_proj.weight\n",
      "Loading weight file layers.9.mlp.up_proj.weight\n",
      "Loading weight file layers.9.mlp.down_proj.weight\n",
      "Loading weight file layers.10.input_layernorm.weight\n",
      "Loading weight file layers.10.self_attn.q_proj.weight\n",
      "Loading weight file layers.10.self_attn.k_proj.weight\n",
      "Loading weight file layers.10.self_attn.v_proj.weight\n",
      "Loading weight file layers.10.self_attn.o_proj.weight\n",
      "Loading weight file layers.10.post_attention_layernorm.weight\n",
      "Loading weight file layers.10.mlp.gate_proj.weight\n",
      "Loading weight file layers.10.mlp.up_proj.weight\n",
      "Loading weight file layers.10.mlp.down_proj.weight\n",
      "Loading weight file layers.11.input_layernorm.weight\n",
      "Loading weight file layers.11.self_attn.q_proj.weight\n",
      "Loading weight file layers.11.self_attn.k_proj.weight\n",
      "Loading weight file layers.11.self_attn.v_proj.weight\n",
      "Loading weight file layers.11.self_attn.o_proj.weight\n",
      "Loading weight file layers.11.post_attention_layernorm.weight\n",
      "Loading weight file layers.11.mlp.gate_proj.weight\n",
      "Loading weight file layers.11.mlp.up_proj.weight\n",
      "Loading weight file layers.11.mlp.down_proj.weight\n",
      "Loading weight file norm.weight\n",
      "Loading weight file lm_head.weight\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 3072, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 768, num_shards: 1, shard_id: 0\n",
      "[0 - 7f3bbc442740]   10.107579 {3}{RequestManager}: [Finetuning] guid(1000000) completed_training_steps(10) processed_finetuning_tokens(457) latency(10107539.0)\n",
      "[0 - 7f3c2ae3f280]   10.110785 {3}{flexflow_c}: [RequestManager] terminate background server 0xa292510\n",
      "2024-07-17 15:41:39 - ###PEFT DEBUGGING### Background serving task completed.\n",
      "Background server stopped.\n"
     ]
    }
   ],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"inference_dataset\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "demo = FlexFlowDemo(configs_dict, mode_only=\"finetuning\")\n",
    "\n",
    "demo.initialize_flexflow()\n",
    "demo.start_server()\n",
    "#demo.generate_inference()\n",
    "demo.generate_finetuning()\n",
    "#demo.generate_inference()\n",
    "demo.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
